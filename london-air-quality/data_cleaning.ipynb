{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "import math\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('DOTSCIENCE_OUTPUTS=[\"input_data\"]')\n",
    "print('DOTSCIENCE_LABELS={\"model_type\": \"data_cleaning\"}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have taken data on the particulates in the air in London from (www.londonair.org.uk)[www.londonair.org.uk].\n",
    "\n",
    "In particular, we chose data on:\n",
    "* Nitric Oxide (NO)\n",
    "* Nitrogen dioxide (N2)\n",
    "* Oxides of nitrogen (NOX) \n",
    "* PM2.5 Particulate (PM2.5)\n",
    "* Sulphur Dioxide (SO2)\n",
    "\n",
    "(all measured in ug/m3).\n",
    "\n",
    "The air was sampled at Haringey Town Hall at 15 minute intervals between 1st June 2018 and 30th June 2018.\n",
    "\n",
    "[Query entered here](https://www.londonair.org.uk/london/asp/datasite.asp?CBXSpecies1=NOm&CBXSpecies2=NO2m&CBXSpecies3=NOXm&CBXSpecies5=PM25m&CBXSpecies6=SO2m&day1=1&month1=jun&year1=2018&day2=30&month2=jun&year2=2018&period=15min&ratidate=&site=HG1&res=6&Submit=Replot+graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('https://www.londonair.org.uk/london/asp/downloadsite.asp?site=HG1&species1=NOm&species2=NO2m&species3=NOXm&species4=PM25m&species5=SO2m&species6=&start=1-jun-2018&end=30-jun-2018&res=6&period=15min&units=ugm3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the raw data, and point Dotscience to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will put the data under version control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('DOTSCIENCE_INPUTS=[\"gov_data_air_quality\"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"raw_air_data_harringey.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are too many rows to view the entire dataframe. The records are sorted by `Species`. Let's see what unique values are in this column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.Species.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are five values: `NO` for nitric oxide, `NO2` for nitrogen dioxide, `NOX` for oxides of nitrogen, `Pm2.5` for PM2.5 Particulate, and `S02` for sulphur dioxide. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How is each particulate measurement recorded? Let's look at the mesasurements for one timestamp. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.loc[df.ReadingDateTime == \"01/06/2018 11:00\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that each unique timehas  five records: one for each of the particulates. Let's check that this is true in general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datetimes_no_data =[]\n",
    "for date in df.ReadingDateTime:\n",
    "    if len(df.loc[df.ReadingDateTime == date]) != 5:\n",
    "        datetimes_no_data.append(date)\n",
    "        print(date)\n",
    "\n",
    "if len(datetimes_no_data) == 0:\n",
    "    print(\"no datetimes without sample data for all particulates: NO, NO2, NOX, PM2.5 and SO2!\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sanity check and update our datatypes\n",
    "\n",
    "We had a quick look at our dataframe above, but let's inspect its datatypes more closely. We want to make sure that any numerical data is stored in a format that we can easily perform calulations with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ReadingDateTime` is saved as an `object`, which in Pandas is a string. Let's save it as a proper date so that we can later on use it in date-like operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['ReadingDateTime'] = pd.to_datetime(df['ReadingDateTime'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Success! Let's save the dataframe to CSV. Dotscience will save it as a new version, so our original CSV of the same name is still accessible if we need it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"raw_air_data_harringey.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Look for null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although every timestamp has a record for the five particulates we are interested in, from the part of the dataframe printed above it looks like a lot of these records are null (`NaN`). \n",
    "\n",
    "We want to remove null particulate `Value`s from our data. First, let's check whether any of the other columns have null values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.isnull().any()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, so we know that it is just the `Value` column that has some nulls. Let's see how many nulls it contains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['Value'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see how these null values are distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Null values by particulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "particulates = [str(array) for array in df.Species.unique()]\n",
    "\n",
    "nulls_and_nonnulls_per_particulate = zip(particulates, \n",
    "                            [len(df.loc[(df.Value.isnull()) & (df.Species == particulate)]) for particulate in particulates],\n",
    "                            [(len(df.loc[(df.Species == particulate)])) for particulate in particulates]\n",
    "                           )\n",
    "\n",
    "\n",
    "print(\"particulate: null values: non-null values:\")\n",
    "for (a, b, c) in nulls_and_nonnulls_per_particulate:\n",
    "    print(a, \"\\t\\t\", b, \"\\t\\t\", c-b)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, all of our records for PM2.5 and SO2 are null. 62 of our records for the remaining particulates are null."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and clean up the null values:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the PM2.5 and SO2 measures are null, so, first, let's remove those records altogether from our dataframe. \n",
    "\n",
    "\n",
    "The remaining particulates just have a few nulls, 62 out of 2784, or about 2%. We can probably do something useful with the remaining 98% of nonnull readings for `NO`, `NO2`, and `NOX`. Instead of treating the nulls as gaps in the data, lets replace those holes with the mean values for each respective particulate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Drop the PM2.5 and SO2 rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rows_to_drop = df.index[df['Species'] == \"SO2\"].tolist() + df.index[df['Species'] == \"PM2.5\"].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(rows_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.drop(df.index[rows_to_drop], inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.Species.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we sucessfully removed all the wholly-null particulates. Now, let's save this dataset. We'll give it a new name to distinguish it from the raw data saved earlier. Dotscience will keep track of the provenance of this new CSV, so we'll be able to easily see where and how it was created: namely, from raw_air_data_harringey.csv, via a transformation in this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"nonnull_air_data_harringey.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Replace sparse nulls with appropriate means\n",
    "Then, we can replace those 62 null NO, NO2 and NOX measurements with their respective particulate `Value` means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NO_rows = df.index[df['Species'] == \"NO\"].tolist()\n",
    "NO2_rows = df.index[df['Species'] == \"NO2\"].tolist()\n",
    "NOX_rows = df.index[df['Species'] == \"NOX\"].tolist()\n",
    "\n",
    "NaN_NO_locs = []\n",
    "NaN_NO2_locs = []\n",
    "NaN_NOX_locs = []\n",
    "\n",
    "for row in NO_rows:\n",
    "    if math.isnan(df.at[row, \"Value\"]):\n",
    "        NaN_NO_locs.append(row)\n",
    "        \n",
    "for row in NO2_rows:\n",
    "    if math.isnan(df.at[row, \"Value\"]):\n",
    "        NaN_NO2_locs.append(row)    \n",
    "        \n",
    "for row in NOX_rows:\n",
    "    if math.isnan(df.at[row, \"Value\"]):\n",
    "        NaN_NOX_locs.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(NaN_NO_locs), len(NaN_NO2_locs), len(NaN_NOX_locs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We've saved the locations of the 62 `NaN` particulate values as lists. Now lets change each of these missing particulate values to the mean value for that particulate over the time sampled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "NO_mean = df.loc[NO_rows][\"Value\"].mean()\n",
    "NO2_mean = df.loc[NO2_rows][\"Value\"].mean()\n",
    "NOX_mean = df.loc[NOX_rows][\"Value\"].mean()\n",
    "\n",
    "# loop through the values, updating the NaNs to the respective mean values\n",
    "for nan_loc in NaN_NO_locs:\n",
    "    df.at[nan_loc, 'Value'] = NO_mean\n",
    "    \n",
    "for nan_loc in NaN_NO2_locs:\n",
    "    df.at[nan_loc, 'Value'] = NO2_mean\n",
    "\n",
    "for nan_loc in NaN_NOX_locs:\n",
    "    df.at[nan_loc, 'Value'] = NOX_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should have removed or updated all the null values. Let's check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['Value'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Let's celebrate by saving the null-free dataframe. We can overwrite the previous `nonnull_air_data_harringey.csv` safe in the knowledge dotsience will save the version history, so we can always roll back to the earlier version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"nonnull_air_data_harringey.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop unnecessary columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a second look at the columns of our dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need to know the `Site` values in this case, since all our records are from the same location. So, let's drop this column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.drop(columns='Site', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, let's save this data once more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"air_data_harringey_streamlined.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## hacky summary stat used to try to get a commit made"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "print('DOTSCIENCE_PARAMETERS=' + json.dumps({\"features\": \"blah\"}))\n",
    "\n",
    "print('DOTSCIENCE_SUMMARY=' + json.dumps({\"thing\": 1}))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
